{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdcdec47",
   "metadata": {},
   "source": [
    "# Advanced Usage\n",
    "\n",
    "**IMPORTANT** - Run the getting started notebook first and make sure your environment is set!\n",
    "\n",
    "- Use [this table](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/reasoning?tabs=python-secure%2Cpy#api--feature-support) for the latest information on supported features. \n",
    "- Visit the [Reasoning Models](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/reasoning?tabs=python%2Cpy) documentation page for API details and code snippets. \n",
    "\n",
    "In this notebook, we'll explore capabilities like structured outputs, developer messages, reasoning effort, and vision support.- \n",
    "\n",
    "<br/>\n",
    "\n",
    "| Characteristic | o1 | o4-mini |\n",
    "|:--- |:---|:---|\n",
    "| Developer Messages    | ✅ | ✅ |\n",
    "| Structured Outputs    | ✅ | ✅ |\n",
    "| Context Window Input  | 200K | 100K |\n",
    "| Context Window Output | 200K | 100K |\n",
    "| Reasoning Effort      | ✅ | ✅ |\n",
    "| Vision Support        | ✅ | ✅ |\n",
    "| Chat Completions API  | ✅ | ✅ |\n",
    "| Responses API         | ✅ |    |\n",
    "| Functions / Tools     | ✅ | ✅ |\n",
    "| max_completion_tokens | ✅ |    |\n",
    "| System messages       | ✅ | ✅ |\n",
    "| Reasoning summary     | ✅ | |\n",
    "| Streaming             | ✅ | |\n",
    "| Model Card | [o4-mini](https://ai.azure.com/explore/models/o4-mini/version/2025-04-16/registry/azure-openai) | [o1](https://ai.azure.com/explore/models/o1/version/2024-12-17/registry/azure-openai)  |\n",
    "| api_version | 2025-04-01-preview | 2025-03-01-preview |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a7b919",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8ce4fb",
   "metadata": {},
   "source": [
    "## 1. Developer Messages\n",
    "\n",
    "Functionally developer messages \"role\": \"developer\" are the same as system messages.\n",
    "However, they are the recommended best practice for setting reasoning goals, persona and context.\n",
    "Don't specify both system and developer messages - that will confuse the model.\n",
    "[Learn more](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/reasoning?tabs=python-secure%2Cpy#developer-messages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b04fa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s a simple, well-documented Python function that sums the even numbers in a list of integers:\n",
      "\n",
      "```python\n",
      "def sum_of_evens(numbers):\n",
      "    \"\"\"\n",
      "    Calculate the sum of all even integers in the provided list.\n",
      "\n",
      "    Parameters:\n",
      "    numbers (list of int): A list of integer values.\n",
      "\n",
      "    Returns:\n",
      "    int: The sum of all even numbers in the list. If there are no evens,\n",
      "         returns 0.\n",
      "    \"\"\"\n",
      "    total = 0\n",
      "    for num in numbers:\n",
      "        # Check if the number is even\n",
      "        if num % 2 == 0:\n",
      "            total += num\n",
      "    return total\n",
      "\n",
      "\n",
      "# Example usage:\n",
      "if __name__ == \"__main__\":\n",
      "    sample_list = [1, 2, 3, 4, 5, 6]\n",
      "    print(f\"Sum of evens in {sample_list} is {sum_of_evens(sample_list)}\")\n",
      "    # Output: Sum of evens in [1, 2, 3, 4, 5, 6] is 12\n",
      "```\n",
      "\n",
      "Explanation:\n",
      "1. We define `sum_of_evens(numbers)` accepting a list of integers.\n",
      "2. We initialize a running total `total = 0`.\n",
      "3. For each integer `num` in the list:\n",
      "   - We check if it’s even (`num % 2 == 0`).\n",
      "   - If it is, we add it to `total`.\n",
      "4. Finally, we return `total`, which will be `0` if no even numbers were found.\n",
      "\n",
      "This function runs in O(n) time where n is the length of the list.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "developer_message = \"\"\"\n",
    "You are a developer assistant. \n",
    "Your task is to help me with Python code.\n",
    "You will receive a prompt that describes the code I want to write.\n",
    "You return well documented code that is easy to understand.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "Write a Python function that takes a list of integers and returns the sum of the even numbers in the list.\n",
    "\"\"\"\n",
    "\n",
    "reasoning_level = \"low\"  # Options: low, medium, high\n",
    "\n",
    "\n",
    "# Set up the Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "    api_version=\"2025-04-01-preview\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o4-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": developer_message},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    "    max_completion_tokens=5000,\n",
    "    reasoning_effort=reasoning_level\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b22c63e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Structured Inputs\n",
    "\n",
    "Structured outputs make a model follow a JSON Schema definition that you provide as part of your inference API call. This is in contrast to the older JSON mode feature, which guaranteed valid JSON would be generated, but was unable to ensure strict adherence to the supplied schema. Structured outputs are recommended for function calling, extracting structured data, and building complex multi-step workflows. [Learn more](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/structured-outputs?tabs=python%2Cdotnet-keys&pivots=programming-language-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb8c2d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='science fair' date='Friday' participants=['Alice', 'Bob']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pydantic import BaseModel\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Set up the Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "    api_version=\"2025-04-01-preview\"\n",
    ")\n",
    "\n",
    "# Define a Pydantic model to represent the event information\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "# Define a custom parser for the chat completion response\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"o4-mini\", \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n",
    "    ],\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "\n",
    "# Get the parsed event information\n",
    "event = completion.choices[0].message.parsed\n",
    "print(event)\n",
    "#print(completion.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6e73cf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Function Calling \n",
    "\n",
    "- Structured Outputs for function calling can be enabled with a single parameter, by supplying strict: true.\n",
    "- Structured outputs are not supported with parallel function calls. When using structured outputs set parallel_tool_calls to false.\n",
    "- [Learn more](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/structured-outputs?tabs=python%2Cdotnet-entra-id&pivots=programming-language-python#function-calling-with-structured-outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8040bf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function(arguments='{\"order_id\":\"12345\"}', name='GetDeliveryDate')\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "from typing import Union\n",
    "from pydantic import BaseModel\n",
    "import openai\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "    api_version=\"2025-04-01-preview\"\n",
    ")\n",
    "\n",
    "class GetDeliveryDate(BaseModel):\n",
    "    order_id: str\n",
    "\n",
    "tools = [openai.pydantic_function_tool(GetDeliveryDate)]\n",
    "\n",
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"You are a helpful customer support assistant. Use the supplied tools to assist the user.\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"Hi, can you tell me the delivery date for my order #12345?\"}) \n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o4-mini\", \n",
    "    messages=messages,\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.tool_calls[0].function)\n",
    "#print(response.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74ca4fa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.a. Vision - Identify\n",
    "Vision-enabled chat models are large multimodal models (LMM) developed by OpenAI that can analyze images and provide textual responses to questions about them. They incorporate both natural language processing and visual understanding. [Learn more](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/gpt-with-vision?tabs=rest)\n",
    "\n",
    "Let's try an image from [the Library of Congress](https://www.loc.gov/free-to-use/lighthouses/) and see if it can reason about the location.\n",
    "\n",
    "![Lighthouse](./assets/lighthouses-2.jpg)\n",
    "\n",
    "<!-- <img src=\"https://www.loc.gov/static/portals/free-to-use/public-domain/lighthouses/lighthouses-2.jpg\" alt=\"NYPL\" width=\"20%\"> -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4156c034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the famous Cape Hatteras Lighthouse on Hatteras Island in North Carolina’s Outer Banks. Key facts:\n",
      "\n",
      "• Appearance: Its 198-ft tower is painted in the distinctive black-and-white spiral “barber-pole” pattern.  \n",
      "• History: The original brick lighthouse on that site dated to 1803. The current tower was completed in 1870, and the spiral day-mark was added in 1873 to distinguish it from neighboring lights.  \n",
      "• Location: It stands within Cape Hatteras National Seashore near the village of Buxton, guarding one of the most treacherous stretches of the Atlantic coast known as the “Graveyard of the Atlantic.”  \n",
      "• Engineering Feat: In 1999, due to severe shoreline erosion, the entire lighthouse—masonry and all—was moved about 2,900 feet inland to protect it from encroaching seas. It remains the tallest brick lighthouse in the United States.  \n",
      "• Today: It is an active Coast Guard aid to navigation, a National Historic Landmark, and a popular visitor attraction for its panoramic seaside views.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from openai import AzureOpenAI\n",
    "import os\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "    api_version=\"2025-04-01-preview\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o4-mini\",\n",
    "    messages=[\n",
    "        { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "        { \"role\": \"user\", \"content\": [  \n",
    "            { \n",
    "                \"type\": \"text\", \n",
    "                \"text\": \"Can you recognize the location? Tell me about it.\" \n",
    "            },\n",
    "            { \n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": \"https://www.loc.gov/static/portals/free-to-use/public-domain/lighthouses/lighthouses-2.jpg\"\n",
    "                }\n",
    "            }\n",
    "        ] } \n",
    "    ],\n",
    "    max_completion_tokens=2000 \n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00131ec2",
   "metadata": {},
   "source": [
    "## 4.b Vision - Decipher\n",
    "\n",
    "Let's try an image from [the Library of Congress](https://www.loc.gov/static/portals/free-to-use/public-domain/presidential-papers) and see if it can recognize handwriting and reason about it.\n",
    "\n",
    "![Decipher](./assets/decipher.jpg)\n",
    "\n",
    "<!-- <img src=\"https://www.loc.gov/static/portals/free-to-use/public-domain/presidential-papers/2.jpg\" alt=\"NYPL\" width=\"20%\"> -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cec9b5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s why Lincoln’s handwritten Gettysburg Address is so famous, in brief:\n",
      "\n",
      "• Historic moment – delivered at the 1863 dedication of Gettysburg’s Soldiers’ National Cemetery, amid the Civil War.  \n",
      "• Extraordinary brevity – just about 270 words, yet perfectly structured and memorable.  \n",
      "• Powerful principles – invokes “all men are created equal,” redefining the war as a fight for liberty and democracy.  \n",
      "• Rhetorical mastery – its plainspoken, rhythmic style set a new standard for public oratory.  \n",
      "• Enduring legacy – continuously memorized, taught, quoted and celebrated as a centerpiece of American identity.\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "import os\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "    api_version=\"2025-04-01-preview\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o4-mini\",\n",
    "    messages=[\n",
    "        { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "        { \"role\": \"user\", \"content\": [  \n",
    "            { \n",
    "                \"type\": \"text\", \n",
    "                \"text\": \"Why is this famous? Summarize it in a short list.\" \n",
    "            },\n",
    "            { \n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": \"https://www.loc.gov/static/portals/free-to-use/public-domain/presidential-papers/2.jpg\"\n",
    "                }\n",
    "            }\n",
    "        ] } \n",
    "    ],\n",
    "    max_completion_tokens=2000 \n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910b5f18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Visual - Local Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091616e4",
   "metadata": {},
   "source": [
    "\n",
    "If you want to use a local image, you can use the following Python code to convert it to base64 so it can be passed to the API. Alternative file conversion tools are available online. Learn more about this and [other settings](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/gpt-with-vision?tabs=python#detail-parameter-settings). Let's try an example with this local image.\n",
    "\n",
    "<img src=\"./assets/lavacake.webp\" alt=\"Lava Cake\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ccd62da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What you’ve got there looks like a chocolate molten (lava) cake with a berry compote, a scoop of vanilla ice cream, and a few fresh strawberries. Roughly speaking:  \n",
      "• Molten chocolate cake (≈100 g): 350–400 kcal  \n",
      "• Vanilla ice cream (≈½ cup or one scoop): 140–180 kcal  \n",
      "• Berry sauce (≈2 Tbsp) + strawberries: 30–50 kcal  \n",
      "\n",
      "All told, you’re in the ballpark of 520–630 kcal. Exact numbers will vary with the recipe and serving size, but that range should give you a reasonable estimate.\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "# Transforming local image to base64\n",
    "image_path = './assets/lavacake.webp'\n",
    "with open(image_path, \"rb\") as image_file:\n",
    "    base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "image_uri = f\"data:image/jpeg;base64,{base64_image}\"\n",
    "\n",
    "# Make a request to the o4-mini model with the image\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o4-mini\",\n",
    "    messages=[\n",
    "        { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "        { \"role\": \"user\", \"content\": [  \n",
    "            { \n",
    "                \"type\": \"text\", \n",
    "                \"text\": \"How many calories are in this ?\" \n",
    "            },\n",
    "            { \n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": image_uri,\n",
    "                    \"detail\" : \"high\"\n",
    "                }       \n",
    "            }\n",
    "        ] } \n",
    "    ],\n",
    "    max_completion_tokens=2000 \n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f474605",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34de4f1",
   "metadata": {},
   "source": [
    "## 6. Your Turn - Try It!\n",
    "\n",
    "Copy one of the above examples and modify it to see how the model responds to different prompts or images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2434d91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your developer message\n",
    "# e.g., \"You are a helpful assistant that uses visual information to solve problems\"\n",
    "developer_message = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Write your prompt \n",
    "# e.g., Copy over the content section from a previous prompt, then change the image URL\n",
    "prompt = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Write your reasoning level\n",
    "reasoning_level = \"low\"  \n",
    "\n",
    "# ---------- Run the cell to see result ------\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o4-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": developer_message},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    "    max_completion_tokens=5000,\n",
    "    reasoning_effort=reasoning_level\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(response.choices[0].message.content)\n",
    "# --------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed726ae",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
